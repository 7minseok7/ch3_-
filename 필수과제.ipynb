{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 이 파일과 동일한 폴더에 자신의 gemini API가 텍스트 형태로 담긴 gemini_api.key 파일을 둘 것.\n",
    "with open('gemini_api.key', 'r') as f:\n",
    "    api_key = f.read()\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 0}, page_content='8 특집원고  초거대 언어모델 연구 동향\\n초거대 언어모델 연구 동향\\n업스테이지  박찬준*･이원성･김윤기･김지후･이활석\\n \\n1. 서  론1)\\nChatGPT1)와 같은 초거대 언어모델(Large Language \\nModel, LLM) 의 등장으로 기존에 병렬적으로 연구되\\n던 다양한 자연언어처리 하위 분야들이 하나의 모델\\n로 처리되고 있으며, 태스크 수렴 현상 (Converge)이 \\n발생하고 있다. 즉 하나의 LLM으로 번역, 요약, 질의\\n응답, 형태소분석 등의 작업을 모두 처리할 수 있게 \\n되었다. 프롬프트 (Prompt)를 어떻게 모델에게 입력하\\n느냐에 따라서 LLM의 다양한 능력들이 창발되고, 이\\n에 따라 사용자의 목적에 맞는 출력을 생성하는 패러\\n다임을 맞이하게 되었다 [1].\\nLLM은 최근 몇 년 간의 연구 동향에 따라 뛰어난 \\n발전을 이루고 있다. 이러한 발전은 몇 가지 주요한 \\n요인에 기반하고 있으며, 이 요인들은 현대 자연언어\\n처리 (Natural Language Processing, NLP) 연구의 핵심\\n적인 추세로 간주된다. 첫째로, 데이터의 양적 확대는 \\n무시할 수 없는 중요한 요인이다. 디지털화의 선도로, \\n텍스트 데이터의 양이 기하급수적으로 증가하였고, \\n이는 연구의 질적 변화를 가져왔다. 대규모 코퍼스의 \\n활용은 LLM의 일반화 능력을 향상시키며, 다양한 맥\\n락과 주제에 대한 깊은 학습을 가능하게 한다. 둘째\\n로, 컴퓨팅 기술의 진보는 LLM의 발전에 있어 결정\\n적이었다. 특히, Graphics Processing Unit (GPU) 및 \\nTensor Processing Unit (TPU) 와 같은 고성능 병렬 처\\n리 하드웨어의 개발은 모델 학습에 있어 병목 현상을 \\n크게 완화시켰다. 이로 인해 연구자들은 모델의 복잡\\n성을 키우고, 더욱 깊은 신경망 구조를 탐구할 수 있\\n게 되었다. 셋째, 알고리즘 및 기술의 발전은 LLM의 \\n성능 향상을 주도하였다. Attention 및 Transformer \\nArchitecture의 도입은 연구자들에게 문맥 간의 관계\\n를 더욱 정교하게 모델링할 수 있는 방법을 제공하였\\n다 [2, 3]. 이 모든 변화의 중심에는 ‘scaling law’라는 \\n* 정회원\\n1) https://openai.com/blog/chatgpt\\n학문적인 통찰이 있다 [4]. 해당 연구에 따르면, 모델\\n의 크기와 그 성능은 긍정적인 상관 관계를 보인다. \\n이를 통해 연구자들은 모델의 파라미터 수를 증가시\\n키면서, 이에 따른 성능 향상을 기술적 진보의 상호 \\n작용에서 나온 결과이며, 이러한 추세는 앞으로도 \\nNLP 연구의 주요 동력이 될 것으로 예상된다.\\n연구단계를 넘어 LLM은 산업계에서도 많은 발전\\n을 이루어 내고 있다. LLM 은 교육, 의료, 금융, 제조 \\n등 거의 모든 산업 분야에서 광범위한 활용 가능성을 \\n제시하고 있다 [5, 6, 7, 8]. 교육 분야에서는 단순한 \\n정보 검색을 넘어, 개인화된 학습 경로를 추천하는 시\\n스템, 과제의 자동 평가, 학생들의 복잡한 질문에 대\\n한 답변 제공 등의 역할로 활용될 수 있다. 이는 교육\\n의 효율성과 개인화를 동시에 추구하는 현대의 교육 \\n트렌드와 맞물려 큰 효과를 발휘할 것으로 기대된다. \\n의료 분야에서는 환자 데이터를 기반으로 한 초기 진\\n단 도구로 활용될 뿐만 아니라, 복잡한 의료 기록 분\\n석, 신약 개발에 필요한 연구 데이터 분석, 또는 최신 \\n의학 연구 동향 파악 등의 다양한 역할을 수행할 수 \\n있다. 이로써 의료 전문가들의 결정을 보조하고, 효율\\n적인 치료 방향을 도모할 수 있게 된다. 금융 분야에\\n서는 개인의 투자 성향과 시장의 동향을 분석하여 투\\n자 권고를 제공하는 것 외에도, 금융 위험을 상세하게 \\n분석하거나, 복잡한 금융 거래를 자동화하는 시스템\\n의 핵심 구성 요소로서의 역할을 할 수 있다. 이는 금\\n융 서비스의 효율과 안전성 향상에 크게 기여할 것이\\n다. 제조 분야에서도 LLM은 설계 단계부터 생산, 품\\n질 관리에 이르기까지의 전 과정에서 데이터 분석 및 \\n최적화 도구로 활용될 수 있다. 생산 효율성 향상과 \\n제품 품질 향상을 도모하며, 고객의 니즈에 더욱 민첩\\n하게 대응할 수 있는 기회를 제공한다.\\n그러나, 이러한 긍정적인 측면들과 더불어 LLM의 \\n한계점과 위험성도 고려되어야 한다. LLM 은 학습 데\\n이터의 편향성을 그대로 반영할 수 있어, 편향된 결과\\n나 추천을 할 가능성이 있다 [9]. 이는 특히 중요한 의\\n특집원고'),\n"
       ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 일반적으로 텍스트 객체가 구분되는 pdf라면 이걸로 읽을 수 있다.\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF 파일 로드. 파일의 경로 입력\n",
    "loader = PyPDFLoader(\"Datasets/초거대_언어모델_연구_동향.pdf\")\n",
    "\n",
    "# 페이지 별 문서 로드\n",
    "docs = loader.load()\n",
    "\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pdf가 스캔본 등의 이유로 텍스트가 아닌 이미지가 나열된 형태의 파일이라면, 아래의 방법으로 실행한다.\n",
    "# import easyocr\n",
    "# reader = easyocr.Reader(['ko','en'])\n",
    "# docs = []  # 페이지 당 인식된 텍스트를 담을 리스트다.\n",
    "# file_name = '2024_세금절약_가이드.pdf'  # 파일 이름을 적는다.\n",
    "\n",
    "# # pdf2image에서 covert_from_path 함수 호출\n",
    "# from pdf2image import convert_from_path\n",
    "# import numpy as np  # 이미지를 넘파이 배열로 변환하기 위해 로드함.\n",
    "\n",
    "# # convert_from_path를 이용해 pdf파일를 이미지형태로 불러온다.\n",
    "# images = convert_from_path('Datasets/' + file_name)\n",
    "\n",
    "# from langchain_core.documents import Document\n",
    "\n",
    "# # images는 여러 페이지로 구성되어 있어 아래와 같이 각각의 페이지(이미지)에 대해 OCR을 수행한다.\n",
    "# for i, image in enumerate(images):\n",
    "#     docs.append(Document(metadata={'source': file_name, 'page': i}, page_content=' '.join(reader.readtext(np.array(image), detail=0))))\n",
    "\n",
    "# docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 분리하기 (Chunking)\n",
    "단어를 벡터로 나타내는 임베딩을 진행하기 전, 큰 텍스트를 작은 단위로 분리하는 작업을 청킹이라고 하고, <br>\n",
    "이로 인해 나눠진 작은 조각 단위를 청크(덩어리)라고 한다.\n",
    "\n",
    "특히 문서가 너무 길어서 벡터로 변환하기 어려울 때, <br>\n",
    "이 방법을 통해 문서의 의미를 최대한 보존하면서 작은 조각으로 나누어 처리하도록 한다. <br>\n",
    "임베딩하고자 하는 문서의 종류와 글의 목적 등에 따라 동일한 청킹 기법을 적용해도 성능이 달라질 수 있다.\n",
    "\n",
    "만일 청크의 크기가 너무 작으면 문맥이 부족할 수 있고, 반대로 너무 크면 검색의 정확도가 감소할 수도 있으므로, <br>\n",
    "적절한 청크 크기 및 기법을 선택하는 것이 중요하다.\n",
    "\n",
    "아래는 텍스트를 덩어리로 쪼개는 두 가지 방식을 나열한 것이다.\n",
    "\n",
    "우선 청킹을 기본적으로 할 수 있는 CharacterTextSplitter 클래스를 보자. 들어가는 인자는 다음과 같다.\n",
    "- separator: 텍스트를 나눌 기준이 되는 문자열을 지정한다. 만약 이 인자가 없다면, 분할 작업을 하지 않는 것으로 보인다.\n",
    "- chunk_size: 청크의 길이를 정한다. <br>\n",
    "위의 seperator로 분할이 가능한 지점 중 chunk_size를 넘지 않는 최대 크기를 갖도록 분할이 진행된다.\n",
    "- chunk_overlap: 청크 사이에 중복으로 포함할 문자 수를 지정할 수 있다. <br>\n",
    "이 경우, 이전 청크의 끝 부분 N개의 글자가 다음 청크의 첫 부분 N개의 글자로 등장하게 된다. <br>\n",
    "중복으로 포함할 문자도 역시 구분자로 구분되어야 하며, 구분자 사이에 있는 문자 개수가 chunk_overlap 미만일 때 위와 같은 효과가 적용된다.\n",
    "- length_function: 청크의 길이를 계산하는 함수 (대부분 파이썬 내장함수인 len을 쓴다.)\n",
    "- is_separator_regex: 구분자가 정규식인지를 나타내는 Boolean 값이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 0}, page_content='8 특집원고  초거대 언어모델 연구 동향\\n초거대 언어모델 연구 동향\\n업스테이지  박찬준*･이원성･김윤기･김지후･이활석\\n \\n1. 서  론1)'),\n",
       " Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 0}, page_content='1. 서  론1)\\nChatGPT1)와 같은 초거대 언어모델(Large Language \\nModel, LLM) 의 등장으로 기존에 병렬적으로 연구되'),\n",
       " Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 0}, page_content='던 다양한 자연언어처리 하위 분야들이 하나의 모델\\n로 처리되고 있으며, 태스크 수렴 현상 (Converge)이 \\n발생하고 있다. 즉 하나의 LLM으로 번역, 요약, 질의'),\n",
       " Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 0}, page_content='응답, 형태소분석 등의 작업을 모두 처리할 수 있게 \\n되었다. 프롬프트 (Prompt)를 어떻게 모델에게 입력하\\n느냐에 따라서 LLM의 다양한 능력들이 창발되고, 이'),\n",
       " Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 0}, page_content='느냐에 따라서 LLM의 다양한 능력들이 창발되고, 이\\n에 따라 사용자의 목적에 맞는 출력을 생성하는 패러\\n다임을 맞이하게 되었다 [1].')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=30,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "splits[:5]  # 앞에 5개만 출력했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 RecursiveCharacterTextSplitter도 비슷하지만, 위와 같이 지정한 문자 기준으로 자르는 것이 아니라, 여러 기준을 순차적으로 적용하며 잘라 나간다. <br>\n",
    "단락, 문장, 단어 단위 순으로 탐색하면서 분할하며 이렇게 하면 문서의 구조를 유지하기가 쉬워진다. (이 이유로 seperator를 따로 지정하지 않아도 된다.) <br>\n",
    "여기서는 상기의 이유로 RecursiveCharacterTextSplitter를 사용하도록 하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 0}, page_content='8 특집원고  초거대 언어모델 연구 동향\\n초거대 언어모델 연구 동향\\n업스테이지  박찬준*･이원성･김윤기･김지후･이활석\\n \\n1. 서  론1)'),\n",
       " Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 0}, page_content='ChatGPT1)와 같은 초거대 언어모델(Large Language \\nModel, LLM) 의 등장으로 기존에 병렬적으로 연구되\\n던 다양한 자연언어처리 하위 분야들이 하나의 모델'),\n",
       " Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 0}, page_content='로 처리되고 있으며, 태스크 수렴 현상 (Converge)이 \\n발생하고 있다. 즉 하나의 LLM으로 번역, 요약, 질의\\n응답, 형태소분석 등의 작업을 모두 처리할 수 있게'),\n",
       " Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 0}, page_content='되었다. 프롬프트 (Prompt)를 어떻게 모델에게 입력하\\n느냐에 따라서 LLM의 다양한 능력들이 창발되고, 이\\n에 따라 사용자의 목적에 맞는 출력을 생성하는 패러'),\n",
       " Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 0}, page_content='다임을 맞이하게 되었다 [1].\\nLLM은 최근 몇 년 간의 연구 동향에 따라 뛰어난 \\n발전을 이루고 있다. 이러한 발전은 몇 가지 주요한')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=2,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "splits = recursive_text_splitter.split_documents(docs)\n",
    "splits[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# gemini의 임베딩 모델을 적용한다. 임베딩은 위에서도 언급했듯 단어를 벡터로 변환하는 과정이다.\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\") # gemini의 임베딩 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISS\n",
    "- FAISS는 Facebook에서 개발 및 배포한 밀집 벡터의 유사도 측정과 클러스터링에 효율적인 라이브러리다.\n",
    "- 여기서 from_documents() 메서드를 사용하면 위에서 만든 Document 객체들을 활용하여 FAISS 벡터 저장소를 생성한다.\n",
    "- 이후 이렇게 저장된 벡터들을 활용하여 유사도를 측정하거나 이를 활용한 비슷한 유사도를 갖는 벡터를 검색하는 일에 활용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_store = FAISS.from_documents(documents=splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벡터 저장소 객체의 메서드인 as_retriever()는 현재 벡터 저장소를 기반으로 VectorStoreRetriever 객체를 생성하도록 한다.\n",
    "- VectorStoreRetriever 객체는 벡터 저장소를 기반으로 하는 검색기 객체이다.\n",
    "- search_type은 유사한 벡터를 찾는 기준이며 여기서는 기본값인 similarity(유사도)를 사용한다.\n",
    "- search_kwargs는 검색 함수에 전달할 추가 키워드 인자로 다음과 같은 것들이 있다. <br><br>\n",
    "    - k: 반환할 문서 수\n",
    "    - score_threshold: 유사도 점수 임계값\n",
    "    - fetch_k: MMR 알고리즘에 전달할 문서 수\n",
    "    - lambda_mult: MMR 다양성 조절 파라미터\n",
    "    - filter: 문서 메타데이터 기반 필터링\n",
    "\n",
    "여기서는 반환할 문서 수인 k값을 조정해 준다. fetch_k와 k값을 적절히 조정하여 최적의 성능을 나오도록 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "contextual_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"반드시 이미 주어져 있는 맥락만을 기반으로 해서 한국어로 답변해 줘. 또한 주의사항은 말하지 마.\"),\n",
    "    (\"user\", \"Context: {context}\\\\n\\\\nQuestion: {question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebugPassThrough(RunnablePassthrough):\n",
    "    def invoke(self, *args, **kwargs):\n",
    "        output = super().invoke(*args, **kwargs)\n",
    "        print(\"Debug Output:\", output)\n",
    "        return output\n",
    "# 문서 리스트를 텍스트로 변환하는 단계 추가\n",
    "class ContextToText(RunnablePassthrough):\n",
    "    def invoke(self, inputs, config=None, **kwargs):  # config 인수 추가\n",
    "        # context의 각 문서를 문자열로 결합\n",
    "        context_text = \"\\n\".join([doc.page_content for doc in inputs[\"context\"]])\n",
    "        return {\"context\": context_text, \"question\": inputs[\"question\"]}\n",
    "\n",
    "# RAG 체인에서 각 단계마다 DebugPassThrough 추가\n",
    "rag_chain_debug = {\n",
    "    \"context\": retriever,                    # 컨텍스트를 가져오는 retriever\n",
    "    \"question\": DebugPassThrough()        # 사용자 질문이 그대로 전달되는지 확인하는 passthrough\n",
    "}  | DebugPassThrough() | ContextToText()|   contextual_prompt | model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================\n",
      "Debug Output: 이 논문에는 무슨 내용이 있는가?\n",
      "Debug Output: {'context': [Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 7}, page_content='등의 내재적인 한계를 지니고 있다. 또한, 자연어 코\\n퍼스를 활용하여 학습되기 때문에, 주요 NLP 태스크\\n가 아닌 산술 추론 (e.g., 1234+4321=?) 등에 약점을'), Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 4}, page_content='연구는 아직 초기 단계에 머물러 있다. 이와 관련하여 \\n주목할 만한 연구로는 [41]이 있다. 해당 연구에서는 \\n사전 학습 코퍼스를 시간대, 필터링 기법, 도메인 혼'), Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 6}, page_content='프롬프트는 자연어 텍스트 형태의 태스크 설명, 시연\\n을 위한 몇 가지 예시 및 테스트 쿼리로 구성된다. 최\\n신 연구 [82]에 따르면, ICL 은 다음과 같은 다양한 이')], 'question': '이 논문에는 무슨 내용이 있는가?'}\n",
      "Final Response:\n",
      "자연어 처리(NLP) 모델은 내재적인 한계를 가지고 있으며, 특히 산술 추론(e.g., 1234+4321=?)에 약하다. 이러한 한계를 극복하기 위한 연구는 아직 초기 단계이다.  [41]에서는 사전 학습 코퍼스를 시간대, 필터링 기법, 도메인 등으로 나누어 학습하는 방법을 제시했고,  [82]에서는 In-Context Learning(ICL)에 대해 다루고 있다. ICL은 자연어 텍스트 형태의 태스크 설명, 시연 예시, 테스트 쿼리로 구성된 프롬프트를 사용한다.\n",
      "\n",
      "========================\n",
      "Debug Output: 혹시 자연어 처리 모델이 무엇인지 설명해 줄 수 있나요?\n",
      "Debug Output: {'context': [Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 4}, page_content='연구는 아직 초기 단계에 머물러 있다. 이와 관련하여 \\n주목할 만한 연구로는 [41]이 있다. 해당 연구에서는 \\n사전 학습 코퍼스를 시간대, 필터링 기법, 도메인 혼'), Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 10}, page_content='18 특집원고  초거대 언어모델 연구 동향\\n버시 보호, 3) 다양성 존중, 4) 침해금지, 5) 공공성, 6) \\n연대성, 7) 데이터 관리, 8) 책임성, 9) 안정성, 10) 투'), Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 6}, page_content='프롬프트는 자연어 텍스트 형태의 태스크 설명, 시연\\n을 위한 몇 가지 예시 및 테스트 쿼리로 구성된다. 최\\n신 연구 [82]에 따르면, ICL 은 다음과 같은 다양한 이')], 'question': '혹시 자연어 처리 모델이 무엇인지 설명해 줄 수 있나요?'}\n",
      "Final Response:\n",
      "제공된 맥락에는 자연어 처리 모델에 대한 정의가 없습니다. 다만, 자연어 텍스트 형태의 태스크 설명, 시연을 위한 예시, 그리고 테스트 쿼리를 포함하는 \"프롬프트\"가 최신 연구([82])에서 ICL과 관련하여 언급되고 있습니다.\n",
      "\n",
      "========================\n",
      "Debug Output: 현재 자연어 처리의 한계는 무엇인가요?\n",
      "Debug Output: {'context': [Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 7}, page_content='등의 내재적인 한계를 지니고 있다. 또한, 자연어 코\\n퍼스를 활용하여 학습되기 때문에, 주요 NLP 태스크\\n가 아닌 산술 추론 (e.g., 1234+4321=?) 등에 약점을'), Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 4}, page_content='연구는 아직 초기 단계에 머물러 있다. 이와 관련하여 \\n주목할 만한 연구로는 [41]이 있다. 해당 연구에서는 \\n사전 학습 코퍼스를 시간대, 필터링 기법, 도메인 혼'), Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 6}, page_content='프롬프트는 자연어 텍스트 형태의 태스크 설명, 시연\\n을 위한 몇 가지 예시 및 테스트 쿼리로 구성된다. 최\\n신 연구 [82]에 따르면, ICL 은 다음과 같은 다양한 이')], 'question': '현재 자연어 처리의 한계는 무엇인가요?'}\n",
      "Final Response:\n",
      "등의 내재적인 한계를 지니고 있으며, 자연어 코퍼스를 활용하여 학습되기 때문에 산술 추론(e.g., 1234+4321=?) 등 주요 NLP 태스크가 아닌 영역에 약점을 보입니다.\n",
      "\n",
      "========================\n",
      "Debug Output: LSTM 모델의 한계는 무엇일까요?\n",
      "Debug Output: {'context': [Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 2}, page_content='향 문맥 정보를 함께 활용하는 양방향 학습을 제안했\\n다. 이를 위해, ELMo 는 순방 향 LSTM과 역방향 \\nLSTM를 동시에 활용한다. 하지만, 이는 LSTM을 기'), Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 2}, page_content='반으로 하기 때문에, LSTM이 지니는 다음과 같은 한\\n계를 그대로 가진다: 1) 하나의 벡터에 텍스트의 모든 \\n정보를 담기 때문에 정보 손실이 발생하고, 2) 입력'), Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 2}, page_content='10 특집원고  초거대 언어모델 연구 동향\\n치하는 정보들을 기억하지 못하는 장기 의존성 문제\\n가 존재한다. 이러한 문제를 극복하기 위해, LSTM')], 'question': 'LSTM 모델의 한계는 무엇일까요?'}\n",
      "Final Response:\n",
      "텍스트의 모든 정보를 하나의 벡터에 담아 정보 손실이 발생하고, 입력된 정보들을 기억하지 못하는 장기 의존성 문제가 있습니다.\n",
      "\n",
      "========================\n",
      "Debug Output: LLM은 현재 어떤 분야에서 활용되고 있나요?\n",
      "Debug Output: {'context': [Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 3}, page_content='원을 활용함으로써, LLM 으로 하여금 세상에 대한 기\\n본 지식 (world knowledge)을 습득할 수 있도록 한다. \\n결과적으로 LLM은 전통적인 언어모델에 비해 뛰어'), Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 3}, page_content='의 LLM을 개발하는 사전 학습 과정과, 하위 태스크\\n를 보다 잘 풀기 위한 목적으로, LLM 을 도메인 또는'), Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 7}, page_content='LLM은 필연적으로 많은 수의 파라미터를 요구하게 \\n되는 것이다. 이러한 내재적인 한계를 해결하고 적은 \\n수의 파라미터로도 목적을 달성할 수 있도록, LLM 을')], 'question': 'LLM은 현재 어떤 분야에서 활용되고 있나요?'}\n",
      "Final Response:\n",
      "주어진 맥락에서는 LLM의 활용 분야에 대한 정보가 없습니다. LLM의 파라미터 수, world knowledge 습득 방법, 하위 태스크 해결 목적 등에 대한 내용만 포함되어 있습니다.\n",
      "\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "# 이제 gemini와 연결해서 대화를 진행해보자.\n",
    "while 1: \n",
    "\tprint(\"========================\")\n",
    "\tquery = input(\"질문을 입력하세요: \")\n",
    "\tresponse = rag_chain_debug.invoke(query)\n",
    "\tprint(\"Final Response:\")\n",
    "\tprint(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "for_gemini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
