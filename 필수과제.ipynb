{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 이 파일과 동일한 폴더에 자신의 gemini API가 텍스트 형태로 담긴 gemini_api.key 파일을 둘 것.\n",
    "with open('gemini_api.key', 'r') as f:\n",
    "    api_key = f.read()\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'Datasets/초거대_언어모델_연구_동향.pdf', 'page': 0}, page_content='8 특집원고  초거대 언어모델 연구 동향\\n초거대 언어모델 연구 동향\\n업스테이지  박찬준*･이원성･김윤기･김지후･이활석\\n \\n1. 서  론1)\\nChatGPT1)와 같은 초거대 언어모델(Large Language \\nModel, LLM) 의 등장으로 기존에 병렬적으로 연구되\\n던 다양한 자연언어처리 하위 분야들이 하나의 모델\\n로 처리되고 있으며, 태스크 수렴 현상 (Converge)이 \\n발생하고 있다. 즉 하나의 LLM으로 번역, 요약, 질의\\n응답, 형태소분석 등의 작업을 모두 처리할 수 있게 \\n되었다. 프롬프트 (Prompt)를 어떻게 모델에게 입력하\\n느냐에 따라서 LLM의 다양한 능력들이 창발되고, 이\\n에 따라 사용자의 목적에 맞는 출력을 생성하는 패러\\n다임을 맞이하게 되었다 [1].\\nLLM은 최근 몇 년 간의 연구 동향에 따라 뛰어난 \\n발전을 이루고 있다. 이러한 발전은 몇 가지 주요한 \\n요인에 기반하고 있으며, 이 요인들은 현대 자연언어\\n처리 (Natural Language Processing, NLP) 연구의 핵심\\n적인 추세로 간주된다. 첫째로, 데이터의 양적 확대는 \\n무시할 수 없는 중요한 요인이다. 디지털화의 선도로, \\n텍스트 데이터의 양이 기하급수적으로 증가하였고, \\n이는 연구의 질적 변화를 가져왔다. 대규모 코퍼스의 \\n활용은 LLM의 일반화 능력을 향상시키며, 다양한 맥\\n락과 주제에 대한 깊은 학습을 가능하게 한다. 둘째\\n로, 컴퓨팅 기술의 진보는 LLM의 발전에 있어 결정\\n적이었다. 특히, Graphics Processing Unit (GPU) 및 \\nTensor Processing Unit (TPU) 와 같은 고성능 병렬 처\\n리 하드웨어의 개발은 모델 학습에 있어 병목 현상을 \\n크게 완화시켰다. 이로 인해 연구자들은 모델의 복잡\\n성을 키우고, 더욱 깊은 신경망 구조를 탐구할 수 있\\n게 되었다. 셋째, 알고리즘 및 기술의 발전은 LLM의 \\n성능 향상을 주도하였다. Attention 및 Transformer \\nArchitecture의 도입은 연구자들에게 문맥 간의 관계\\n를 더욱 정교하게 모델링할 수 있는 방법을 제공하였\\n다 [2, 3]. 이 모든 변화의 중심에는 ‘scaling law’라는 \\n* 정회원\\n1) https://openai.com/blog/chatgpt\\n학문적인 통찰이 있다 [4]. 해당 연구에 따르면, 모델\\n의 크기와 그 성능은 긍정적인 상관 관계를 보인다. \\n이를 통해 연구자들은 모델의 파라미터 수를 증가시\\n키면서, 이에 따른 성능 향상을 기술적 진보의 상호 \\n작용에서 나온 결과이며, 이러한 추세는 앞으로도 \\nNLP 연구의 주요 동력이 될 것으로 예상된다.\\n연구단계를 넘어 LLM은 산업계에서도 많은 발전\\n을 이루어 내고 있다. LLM 은 교육, 의료, 금융, 제조 \\n등 거의 모든 산업 분야에서 광범위한 활용 가능성을 \\n제시하고 있다 [5, 6, 7, 8]. 교육 분야에서는 단순한 \\n정보 검색을 넘어, 개인화된 학습 경로를 추천하는 시\\n스템, 과제의 자동 평가, 학생들의 복잡한 질문에 대\\n한 답변 제공 등의 역할로 활용될 수 있다. 이는 교육\\n의 효율성과 개인화를 동시에 추구하는 현대의 교육 \\n트렌드와 맞물려 큰 효과를 발휘할 것으로 기대된다. \\n의료 분야에서는 환자 데이터를 기반으로 한 초기 진\\n단 도구로 활용될 뿐만 아니라, 복잡한 의료 기록 분\\n석, 신약 개발에 필요한 연구 데이터 분석, 또는 최신 \\n의학 연구 동향 파악 등의 다양한 역할을 수행할 수 \\n있다. 이로써 의료 전문가들의 결정을 보조하고, 효율\\n적인 치료 방향을 도모할 수 있게 된다. 금융 분야에\\n서는 개인의 투자 성향과 시장의 동향을 분석하여 투\\n자 권고를 제공하는 것 외에도, 금융 위험을 상세하게 \\n분석하거나, 복잡한 금융 거래를 자동화하는 시스템\\n의 핵심 구성 요소로서의 역할을 할 수 있다. 이는 금\\n융 서비스의 효율과 안전성 향상에 크게 기여할 것이\\n다. 제조 분야에서도 LLM은 설계 단계부터 생산, 품\\n질 관리에 이르기까지의 전 과정에서 데이터 분석 및 \\n최적화 도구로 활용될 수 있다. 생산 효율성 향상과 \\n제품 품질 향상을 도모하며, 고객의 니즈에 더욱 민첩\\n하게 대응할 수 있는 기회를 제공한다.\\n그러나, 이러한 긍정적인 측면들과 더불어 LLM의 \\n한계점과 위험성도 고려되어야 한다. LLM 은 학습 데\\n이터의 편향성을 그대로 반영할 수 있어, 편향된 결과\\n나 추천을 할 가능성이 있다 [9]. 이는 특히 중요한 의\\n특집원고')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 일반적으로 텍스트 객체가 구분되는 pdf라면 이걸로 읽을 수 있다.\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF 파일 로드. 파일의 경로 입력\n",
    "loader = PyPDFLoader(\"Datasets/초거대_언어모델_연구_동향.pdf\")\n",
    "\n",
    "# 페이지 별 문서 로드\n",
    "docs = loader.load()\n",
    "\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pdf가 스캔본 등의 이유로 텍스트가 아닌 이미지가 나열된 형태의 파일이라면, 아래의 방법으로 실행한다.\n",
    "# import easyocr\n",
    "# reader = easyocr.Reader(['ko','en'])\n",
    "# docs = []  # 페이지 당 인식된 텍스트를 담을 리스트다.\n",
    "# file_name = '2024_세금절약_가이드.pdf'  # 파일 이름을 적는다.\n",
    "\n",
    "# # pdf2image에서 covert_from_path 함수 호출\n",
    "# from pdf2image import convert_from_path\n",
    "# import numpy as np  # 이미지를 넘파이 배열로 변환하기 위해 로드함.\n",
    "\n",
    "# # convert_from_path를 이용해 pdf파일를 이미지형태로 불러온다.\n",
    "# images = convert_from_path('Datasets/' + file_name)\n",
    "\n",
    "# from langchain_core.documents import Document\n",
    "\n",
    "# # images는 여러 페이지로 구성되어 있어 아래와 같이 각각의 페이지(이미지)에 대해 OCR을 수행한다.\n",
    "# for i, image in enumerate(images):\n",
    "#     docs.append(Document(metadata={'source': file_name, 'page': i}, page_content=' '.join(reader.readtext(np.array(image), detail=0))))\n",
    "\n",
    "# docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 분리하기 (Chunking)\n",
    "단어를 벡터로 나타내는 임베딩을 진행하기 전, 큰 텍스트를 작은 단위로 분리하는 작업을 청킹이라고 하고, <br>\n",
    "이로 인해 나눠진 작은 조각 단위를 청크(덩어리)라고 한다.\n",
    "\n",
    "특히 문서가 너무 길어서 벡터로 변환하기 어려울 때, <br>\n",
    "이 방법을 통해 문서의 의미를 최대한 보존하면서 작은 조각으로 나누어 처리하도록 한다. <br>\n",
    "임베딩하고자 하는 문서의 종류와 글의 목적 등에 따라 동일한 청킹 기법을 적용해도 성능이 달라질 수 있다.\n",
    "\n",
    "만일 청크의 크기가 너무 작으면 문맥이 부족할 수 있고, 반대로 너무 크면 검색의 정확도가 감소할 수도 있으므로, <br>\n",
    "적절한 청크 크기 및 기법을 선택하는 것이 중요하다.\n",
    "\n",
    "아래는 텍스트를 덩어리로 쪼개는 두 가지 방식을 나열한 것이다.\n",
    "\n",
    "우선 청킹을 기본적으로 할 수 있는 CharacterTextSplitter 클래스를 보자. 들어가는 인자는 다음과 같다.\n",
    "- separator: 텍스트를 나눌 기준이 되는 문자열을 지정한다. 만약 이 인자가 없다면, 분할 작업을 하지 않는 것으로 보인다.\n",
    "- chunk_size: 청크의 길이를 정한다. <br>\n",
    "위의 seperator로 분할이 가능한 지점 중 chunk_size를 넘지 않는 최대 크기를 갖도록 분할이 진행된다.\n",
    "- chunk_overlap: 청크 사이에 중복으로 포함할 문자 수를 지정할 수 있다. <br>\n",
    "이 경우, 이전 청크의 끝 부분 N개의 글자가 다음 청크의 첫 부분 N개의 글자로 등장하게 된다. <br>\n",
    "중복으로 포함할 문자도 역시 구분자로 구분되어야 하며, 구분자 사이에 있는 문자 개수가 chunk_overlap 미만일 때 위와 같은 효과가 적용된다.\n",
    "- length_function: 청크의 길이를 계산하는 함수 (대부분 파이썬 내장함수인 len을 쓴다.)\n",
    "- is_separator_regex: 구분자가 정규식인지를 나타내는 Boolean 값이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Datasets/초거대_언어모델_연구_동향.pdf', 'page': 0}, page_content='8 특집원고  초거대 언어모델 연구 동향\\n초거대 언어모델 연구 동향\\n업스테이지  박찬준*･이원성･김윤기･김지후･이활석\\n \\n1. 서  론1)'),\n",
       " Document(metadata={'source': 'Datasets/초거대_언어모델_연구_동향.pdf', 'page': 0}, page_content='1. 서  론1)\\nChatGPT1)와 같은 초거대 언어모델(Large Language \\nModel, LLM) 의 등장으로 기존에 병렬적으로 연구되'),\n",
       " Document(metadata={'source': 'Datasets/초거대_언어모델_연구_동향.pdf', 'page': 0}, page_content='던 다양한 자연언어처리 하위 분야들이 하나의 모델\\n로 처리되고 있으며, 태스크 수렴 현상 (Converge)이 \\n발생하고 있다. 즉 하나의 LLM으로 번역, 요약, 질의'),\n",
       " Document(metadata={'source': 'Datasets/초거대_언어모델_연구_동향.pdf', 'page': 0}, page_content='응답, 형태소분석 등의 작업을 모두 처리할 수 있게 \\n되었다. 프롬프트 (Prompt)를 어떻게 모델에게 입력하\\n느냐에 따라서 LLM의 다양한 능력들이 창발되고, 이'),\n",
       " Document(metadata={'source': 'Datasets/초거대_언어모델_연구_동향.pdf', 'page': 0}, page_content='느냐에 따라서 LLM의 다양한 능력들이 창발되고, 이\\n에 따라 사용자의 목적에 맞는 출력을 생성하는 패러\\n다임을 맞이하게 되었다 [1].')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=30,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "splits[:5]  # 앞에 5개만 출력했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 RecursiveCharacterTextSplitter도 비슷하지만, 위와 같이 지정한 문자 기준으로 자르는 것이 아니라, 여러 기준을 순차적으로 적용하며 잘라 나간다. <br>\n",
    "단락, 문장, 단어 단위 순으로 탐색하면서 분할하며 이렇게 하면 문서의 구조를 유지하기가 쉬워진다. (이 이유로 seperator를 따로 지정하지 않아도 된다.) <br>\n",
    "여기서는 상기의 이유로 RecursiveCharacterTextSplitter를 사용하도록 하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Datasets/초거대_언어모델_연구_동향.pdf', 'page': 0}, page_content='8 특집원고  초거대 언어모델 연구 동향\\n초거대 언어모델 연구 동향\\n업스테이지  박찬준*･이원성･김윤기･김지후･이활석\\n \\n1. 서  론1)'),\n",
       " Document(metadata={'source': 'Datasets/초거대_언어모델_연구_동향.pdf', 'page': 0}, page_content='ChatGPT1)와 같은 초거대 언어모델(Large Language \\nModel, LLM) 의 등장으로 기존에 병렬적으로 연구되\\n던 다양한 자연언어처리 하위 분야들이 하나의 모델'),\n",
       " Document(metadata={'source': 'Datasets/초거대_언어모델_연구_동향.pdf', 'page': 0}, page_content='로 처리되고 있으며, 태스크 수렴 현상 (Converge)이 \\n발생하고 있다. 즉 하나의 LLM으로 번역, 요약, 질의\\n응답, 형태소분석 등의 작업을 모두 처리할 수 있게'),\n",
       " Document(metadata={'source': 'Datasets/초거대_언어모델_연구_동향.pdf', 'page': 0}, page_content='되었다. 프롬프트 (Prompt)를 어떻게 모델에게 입력하\\n느냐에 따라서 LLM의 다양한 능력들이 창발되고, 이\\n에 따라 사용자의 목적에 맞는 출력을 생성하는 패러'),\n",
       " Document(metadata={'source': 'Datasets/초거대_언어모델_연구_동향.pdf', 'page': 0}, page_content='다임을 맞이하게 되었다 [1].\\nLLM은 최근 몇 년 간의 연구 동향에 따라 뛰어난 \\n발전을 이루고 있다. 이러한 발전은 몇 가지 주요한')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=2,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "splits = recursive_text_splitter.split_documents(docs)\n",
    "splits[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# gemini의 임베딩 모델을 적용한다. 임베딩은 위에서도 언급했듯 단어를 벡터로 변환하는 과정이다.\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\") # gemini의 임베딩 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISS\n",
    "- FAISS는 Facebook에서 개발 및 배포한 밀집 벡터의 유사도 측정과 클러스터링에 효율적인 라이브러리다.\n",
    "- 여기서 from_documents() 메서드를 사용하면 위에서 만든 Document 객체들을 활용하여 FAISS 벡터 저장소를 생성한다.\n",
    "- 이후 이렇게 저장된 벡터들을 활용하여 유사도를 측정하거나 이를 활용한 비슷한 유사도를 갖는 벡터를 검색하는 일에 활용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_store = FAISS.from_documents(documents=splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벡터 저장소 객체의 메서드인 as_retriever()는 현재 벡터 저장소를 기반으로 VectorStoreRetriever 객체를 생성하도록 한다.\n",
    "- VectorStoreRetriever 객체는 벡터 저장소를 기반으로 하는 검색기 객체이다.\n",
    "- search_type은 유사한 벡터를 찾는 기준이며 여기서는 기본값인 similarity(유사도)를 사용한다.\n",
    "- search_kwargs는 검색 함수에 전달할 추가 키워드 인자로 다음과 같은 것들이 있다. <br><br>\n",
    "    - k: 반환할 문서 수\n",
    "    - score_threshold: 유사도 점수 임계값\n",
    "    - fetch_k: MMR 알고리즘에 전달할 문서 수\n",
    "    - lambda_mult: MMR 다양성 조절 파라미터\n",
    "    - filter: 문서 메타데이터 기반 필터링\n",
    "\n",
    "여기서는 반환할 문서 수인 k값을 조정해 준다. fetch_k와 k값을 적절히 조정하여 최적의 성능을 나오도록 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "contextual_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"반드시 이미 주어져 있는 맥락만을 기반으로 해서 한국어로 답변해 줘. 또한 주의사항은 말하지 마.\"),\n",
    "    (\"user\", \"Context: {context}\\\\n\\\\nQuestion: {question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebugPassThrough(RunnablePassthrough):\n",
    "    def invoke(self, *args, **kwargs):\n",
    "        output = super().invoke(*args, **kwargs)\n",
    "        print(\"Debug Output:\", output)\n",
    "        return output\n",
    "# 문서 리스트를 텍스트로 변환하는 단계 추가\n",
    "class ContextToText(RunnablePassthrough):\n",
    "    def invoke(self, inputs, config=None, **kwargs):  # config 인수 추가\n",
    "        # context의 각 문서를 문자열로 결합\n",
    "        context_text = \"\\n\".join([doc.page_content for doc in inputs[\"context\"]])\n",
    "        return {\"context\": context_text, \"question\": inputs[\"question\"]}\n",
    "\n",
    "# RAG 체인에서 각 단계마다 DebugPassThrough 추가\n",
    "# 아래에 파이프(|)로 연결된 클래스들을 순차적으로 통과한다.\n",
    "rag_chain_debug = {\n",
    "    \"context\": retriever,                    # 컨텍스트를 가져오는 retriever\n",
    "    \"question\": DebugPassThrough()        # 사용자 질문이 그대로 전달되는지 확인하는 passthrough\n",
    "} | DebugPassThrough() | ContextToText() | contextual_prompt | model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================\n",
      "Debug Output: 이 논문에는 무슨 내용이 있는가?\n",
      "Debug Output: {'context': [Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 7}, page_content='등의 내재적인 한계를 지니고 있다. 또한, 자연어 코\\n퍼스를 활용하여 학습되기 때문에, 주요 NLP 태스크\\n가 아닌 산술 추론 (e.g., 1234+4321=?) 등에 약점을'), Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 4}, page_content='연구는 아직 초기 단계에 머물러 있다. 이와 관련하여 \\n주목할 만한 연구로는 [41]이 있다. 해당 연구에서는 \\n사전 학습 코퍼스를 시간대, 필터링 기법, 도메인 혼'), Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 6}, page_content='프롬프트는 자연어 텍스트 형태의 태스크 설명, 시연\\n을 위한 몇 가지 예시 및 테스트 쿼리로 구성된다. 최\\n신 연구 [82]에 따르면, ICL 은 다음과 같은 다양한 이')], 'question': '이 논문에는 무슨 내용이 있는가?'}\n",
      "Final Response:\n",
      "자연어 처리(NLP) 모델은 내재적인 한계를 가지고 있으며, 특히 산술 추론(e.g., 1234+4321=?)에 약하다. 이러한 한계를 극복하기 위한 연구는 아직 초기 단계이다.  [41]에서는 사전 학습 코퍼스를 시간대, 필터링 기법, 도메인 등으로 나누어 학습하는 방법을 제시했고,  [82]에서는 In-Context Learning(ICL)에 대해 다루고 있다. ICL은 자연어 텍스트 형태의 태스크 설명, 시연 예시, 테스트 쿼리로 구성된 프롬프트를 사용한다.\n",
      "\n",
      "========================\n",
      "Debug Output: 혹시 자연어 처리 모델이 무엇인지 설명해 줄 수 있나요?\n",
      "Debug Output: {'context': [Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 4}, page_content='연구는 아직 초기 단계에 머물러 있다. 이와 관련하여 \\n주목할 만한 연구로는 [41]이 있다. 해당 연구에서는 \\n사전 학습 코퍼스를 시간대, 필터링 기법, 도메인 혼'), Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 10}, page_content='18 특집원고  초거대 언어모델 연구 동향\\n버시 보호, 3) 다양성 존중, 4) 침해금지, 5) 공공성, 6) \\n연대성, 7) 데이터 관리, 8) 책임성, 9) 안정성, 10) 투'), Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 6}, page_content='프롬프트는 자연어 텍스트 형태의 태스크 설명, 시연\\n을 위한 몇 가지 예시 및 테스트 쿼리로 구성된다. 최\\n신 연구 [82]에 따르면, ICL 은 다음과 같은 다양한 이')], 'question': '혹시 자연어 처리 모델이 무엇인지 설명해 줄 수 있나요?'}\n",
      "Final Response:\n",
      "제공된 맥락에는 자연어 처리 모델에 대한 정의가 없습니다. 다만, 자연어 텍스트 형태의 태스크 설명, 시연을 위한 예시, 그리고 테스트 쿼리를 포함하는 \"프롬프트\"가 최신 연구([82])에서 ICL과 관련하여 언급되고 있습니다.\n",
      "\n",
      "========================\n",
      "Debug Output: 현재 자연어 처리의 한계는 무엇인가요?\n",
      "Debug Output: {'context': [Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 7}, page_content='등의 내재적인 한계를 지니고 있다. 또한, 자연어 코\\n퍼스를 활용하여 학습되기 때문에, 주요 NLP 태스크\\n가 아닌 산술 추론 (e.g., 1234+4321=?) 등에 약점을'), Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 4}, page_content='연구는 아직 초기 단계에 머물러 있다. 이와 관련하여 \\n주목할 만한 연구로는 [41]이 있다. 해당 연구에서는 \\n사전 학습 코퍼스를 시간대, 필터링 기법, 도메인 혼'), Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 6}, page_content='프롬프트는 자연어 텍스트 형태의 태스크 설명, 시연\\n을 위한 몇 가지 예시 및 테스트 쿼리로 구성된다. 최\\n신 연구 [82]에 따르면, ICL 은 다음과 같은 다양한 이')], 'question': '현재 자연어 처리의 한계는 무엇인가요?'}\n",
      "Final Response:\n",
      "등의 내재적인 한계를 지니고 있으며, 자연어 코퍼스를 활용하여 학습되기 때문에 산술 추론(e.g., 1234+4321=?) 등 주요 NLP 태스크가 아닌 영역에 약점을 보입니다.\n",
      "\n",
      "========================\n",
      "Debug Output: LSTM 모델의 한계는 무엇일까요?\n",
      "Debug Output: {'context': [Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 2}, page_content='향 문맥 정보를 함께 활용하는 양방향 학습을 제안했\\n다. 이를 위해, ELMo 는 순방 향 LSTM과 역방향 \\nLSTM를 동시에 활용한다. 하지만, 이는 LSTM을 기'), Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 2}, page_content='반으로 하기 때문에, LSTM이 지니는 다음과 같은 한\\n계를 그대로 가진다: 1) 하나의 벡터에 텍스트의 모든 \\n정보를 담기 때문에 정보 손실이 발생하고, 2) 입력'), Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 2}, page_content='10 특집원고  초거대 언어모델 연구 동향\\n치하는 정보들을 기억하지 못하는 장기 의존성 문제\\n가 존재한다. 이러한 문제를 극복하기 위해, LSTM')], 'question': 'LSTM 모델의 한계는 무엇일까요?'}\n",
      "Final Response:\n",
      "텍스트의 모든 정보를 하나의 벡터에 담아 정보 손실이 발생하고, 입력된 정보들을 기억하지 못하는 장기 의존성 문제가 있습니다.\n",
      "\n",
      "========================\n",
      "Debug Output: LLM은 현재 어떤 분야에서 활용되고 있나요?\n",
      "Debug Output: {'context': [Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 3}, page_content='원을 활용함으로써, LLM 으로 하여금 세상에 대한 기\\n본 지식 (world knowledge)을 습득할 수 있도록 한다. \\n결과적으로 LLM은 전통적인 언어모델에 비해 뛰어'), Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 3}, page_content='의 LLM을 개발하는 사전 학습 과정과, 하위 태스크\\n를 보다 잘 풀기 위한 목적으로, LLM 을 도메인 또는'), Document(metadata={'source': '초거대_언어모델_연구_동향.pdf', 'page': 7}, page_content='LLM은 필연적으로 많은 수의 파라미터를 요구하게 \\n되는 것이다. 이러한 내재적인 한계를 해결하고 적은 \\n수의 파라미터로도 목적을 달성할 수 있도록, LLM 을')], 'question': 'LLM은 현재 어떤 분야에서 활용되고 있나요?'}\n",
      "Final Response:\n",
      "주어진 맥락에서는 LLM의 활용 분야에 대한 정보가 없습니다. LLM의 파라미터 수, world knowledge 습득 방법, 하위 태스크 해결 목적 등에 대한 내용만 포함되어 있습니다.\n",
      "\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "# 이제 gemini와 연결해서 대화를 진행해보자.\n",
    "while 1: \n",
    "\tprint(\"========================\")\n",
    "\tquery = input(\"질문을 입력하세요: \")\n",
    "\tresponse = rag_chain_debug.invoke(query)\n",
    "\tprint(\"Final Response:\")\n",
    "\tprint(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================\n",
      "Debug Output: 자연어 처리 모델이 무엇인지 설명해 줄 수 있나요?\n",
      "Final Response:\n",
      "자연어 처리(Natural Language Processing, NLP) 모델은 인간의 언어를 이해하고 처리하며 생성할 수 있도록 설계된 컴퓨터 프로그램입니다.  쉽게 말해, 컴퓨터가 사람의 말을 알아듣고, 그에 맞는 반응을 하도록 돕는 기술이라고 할 수 있습니다.\n",
      "\n",
      "NLP 모델은 다양한 작업을 수행할 수 있는데, 주요 기능은 다음과 같습니다:\n",
      "\n",
      "* **텍스트 이해:** 문장의 의미, 감정, 의도 등을 파악합니다. 예를 들어, 제품 리뷰에서 고객의 만족도를 분석하거나, 뉴스 기사의 주요 내용을 요약할 수 있습니다.\n",
      "* **텍스트 생성:** 이메일, 기사, 이야기 등 새로운 텍스트를 생성합니다. 예를 들어, 챗봇이 고객의 질문에 답변하거나, 자동으로 기사를 작성하는 데 사용될 수 있습니다.\n",
      "* **번역:** 한 언어를 다른 언어로 번역합니다. 예를 들어, 한국어를 영어로, 영어를 일본어로 번역하는 데 사용됩니다.\n",
      "* **음성 인식 및 합성:** 음성을 텍스트로 변환하거나 텍스트를 음성으로 변환합니다. 예를 들어, 음성 비서가 사용자의 음성 명령을 이해하고 실행하거나, 텍스트를 읽어주는 프로그램에서 사용됩니다.\n",
      "* **맞춤법 및 문법 검사:** 텍스트에서 맞춤법이나 문법 오류를 찾아 수정합니다.\n",
      "* **정보 검색:** 방대한 양의 텍스트 데이터에서 특정 정보를 검색합니다. 예를 들어, 검색 엔진에서 사용자의 검색어에 맞는 웹 페이지를 찾는 데 사용됩니다.\n",
      "\n",
      "NLP 모델은 머신 러닝과 딥 러닝 기술을 기반으로 합니다.  대량의 텍스트 데이터를 학습하여 언어의 패턴과 규칙을 이해하고, 이를 바탕으로 새로운 텍스트를 처리하고 생성합니다.  대표적인 NLP 모델로는 다음과 같은 것들이 있습니다:\n",
      "\n",
      "* **BERT (Bidirectional Encoder Representations from Transformers):** Google에서 개발한 모델로, 문맥을 양방향으로 이해하여 높은 성능을 보입니다.\n",
      "* **GPT (Generative Pre-trained Transformer):** OpenAI에서 개발한 모델로, 텍스트 생성에 뛰어난 성능을 보입니다.\n",
      "* **Transformer:**  BERT와 GPT의 기반이 되는 모델로,  병렬 처리를 통해 학습 속도를 높이고 성능을 향상시켰습니다.\n",
      "\n",
      "\n",
      "NLP는 우리 주변에서 이미 널리 사용되고 있으며, 앞으로 더욱 발전하여 인간과 컴퓨터의 상호 작용을 더욱 자연스럽고 효율적으로 만들어줄 것으로 기대됩니다.\n",
      "\n",
      "========================\n",
      "Debug Output: 현재 자연어 처리의 한계는 무엇인가요?\n",
      "Final Response:\n",
      "현재 자연어 처리(NLP)는 놀라운 발전을 이루었지만, 여전히 몇 가지 중요한 한계점을 가지고 있습니다.\n",
      "\n",
      "**1. 상식 및 추론 능력 부족:**\n",
      "\n",
      "* **암묵적 정보 이해의 어려움:** NLP 모델은 문맥에 명시적으로 드러나지 않은 정보나 상식을 이용하여 추론하는 데 어려움을 겪습니다.  사람은 쉽게 이해할 수 있는 비유, 풍자, 반어법 등을 이해하지 못하는 경우가 많습니다.\n",
      "* **복잡한 추론 및 논리적 사고의 한계:**  단순한 질의응답이나 요약은 잘 수행하지만, 여러 단계의 추론이나 논리적 사고가 필요한 작업에서는 성능이 떨어집니다. 예를 들어, 인과 관계 파악, 가정 추론, 복잡한 문제 해결 등이 어렵습니다.\n",
      "\n",
      "**2. 데이터 의존성 및 편향:**\n",
      "\n",
      "* **대량의 학습 데이터 필요:**  NLP 모델은 대량의 데이터로 학습되어야 좋은 성능을 보입니다.  데이터가 부족한 특정 분야나 언어에서는 성능이 제한적입니다.\n",
      "* **학습 데이터의 편향 반영:** 학습 데이터에 존재하는 편향 (예: 성별, 인종, 지역 등에 대한 편견)이 모델에 반영되어 불공정하거나 차별적인 결과를 초래할 수 있습니다.\n",
      "\n",
      "**3. 문맥 이해의 한계:**\n",
      "\n",
      "* **긴 문맥 유지의 어려움:**  긴 문서나 대화에서 앞부분의 정보를 기억하고 활용하는 데 어려움이 있습니다.  문맥 창의 크기 제한으로 인해 긴 맥락을 완전히 이해하지 못할 수 있습니다.\n",
      "* **다의어 및 중의성 해소의 어려움:**  같은 단어가 여러 의미를 가지는 경우, 문맥에 따라 적절한 의미를 파악하는 데 어려움을 겪습니다.  중의적인 문장의 의미를 정확하게 해석하지 못하는 경우가 발생합니다.\n",
      "\n",
      "**4. 감정 및 의도 파악의 어려움:**\n",
      "\n",
      "* **미묘한 감정 표현 이해의 한계:**  비꼬는 말, 풍자, 빈정거림 등 미묘한 감정 표현을 이해하는 데 어려움을 겪습니다.  단순한 긍정/부정 판단은 가능하지만, 복잡한 감정을 정확하게 파악하기는 어렵습니다.\n",
      "* **발화 의도 파악의 어려움:**  같은 문장이라도 상황에 따라 다른 의도를 가질 수 있습니다.  NLP 모델은 문장의 표면적인 의미만 이해하고, 화자의 의도를 정확하게 파악하지 못하는 경우가 있습니다.\n",
      "\n",
      "**5. 실세계 지식 및 경험 부족:**\n",
      "\n",
      "* **현실 세계에 대한 이해 부족:** NLP 모델은 텍스트 데이터로만 학습되기 때문에, 실제 세계에 대한 경험이나 지식이 부족합니다.  이로 인해 상식에 어긋나는 답변을 생성하거나 현실적인 상황에 대한 이해가 부족한 모습을 보일 수 있습니다.\n",
      "\n",
      "**6. 설명 가능성 부족:**\n",
      "\n",
      "* **블랙박스 모델:**  많은 NLP 모델은 복잡한 신경망 구조를 사용하기 때문에, 모델이 특정 결정을 내린 이유를 설명하기 어렵습니다.  이러한 설명 가능성 부족은 모델의 신뢰성을 저하시키고, 오류 분석 및 개선을 어렵게 만듭니다.\n",
      "\n",
      "\n",
      "이러한 한계점들을 극복하기 위해,  상식 주입, 추론 능력 향상, 데이터 편향 해소, 문맥 이해 강화, 감정 분석 및 의도 파악 개선, 설명 가능한 AI 등 다양한 연구가 진행되고 있습니다.  NLP는 계속해서 발전하고 있으며, 앞으로 더욱 인간과 유사한 자연어 이해 능력을 갖추게 될 것으로 기대됩니다.\n",
      "\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "normal_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"Question: {question}\")\n",
    "])\n",
    "\n",
    "normal_chain = {'question': DebugPassThrough()} | normal_prompt | model\n",
    "\n",
    "# 자료를 넘기지 않고 그냥 대화해 보자.\n",
    "while 1: \n",
    "\tprint(\"========================\")\n",
    "\tquery = input(\"질문을 입력하세요: \")\n",
    "\tresponse = normal_chain.invoke(query)\n",
    "\tprint(\"Final Response:\")\n",
    "\tprint(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 논문을 넘겼을 때의 답변과 아래의 논문을 넘기지 않은 일반적인 상태에서의 답변을 비교하면, <br>\n",
    "위의 경우 논문을 참고해서 답변하는 것이므로 주어진 맥락 안에서 전문적인 답변이 가능한 것을 알 수 있지만, <br>\n",
    "아래의 경우 내용은 친절하지만 사용자가 누구든 상관 없이 보편적인 지식 수준에서 답변을 한다.\n",
    "\n",
    "또한 논문을 데이터로 주고 system 프롬프트로 주어진 맥락 내에서만 답변하라고 했을 때는 실행시간이 굉장히 짧은 특징을 보인다. <br>\n",
    "그러나 일반적인 경우 전체 데이터 혹은 외부에서의 검색을 해야하므로 실행시간이 크게 증가하는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "for_gemini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
